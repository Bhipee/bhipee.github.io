[ğŸŒ‡](echo/) Hi thereğŸ‘‹ğŸ», I am Zheng Liu, a research assistant at [PolyU](https://polyu.edu.hk)<img src='./images/polyu.png' style='width: 1.5em;'> and [InfiX.ai](https://huggingface.co/InfiX-ai)<img src='./images/infix.webp' style='width: 1.5em;'>. Currently, I am supervised by Prof. [Hongxia Yang](https://www4.comp.polyu.edu.hk/~hongxyang/) with a group of talented peers, working on developing GenAI that everyone can access.
I am now leading an effort on MLLM, MoE, Transfer Learning, Gaze Estimation and Hand Pose Estimation. If you are seeking any form of **academic cooperation**, please feel free to email me at [liu_zheng[at]buaa.edu.cn](liu_zheng[at]buaa.edu.cn).

I got my M.Eng degree from Beihang University, advised by Prof. [Feng Lu](http://shi.buaa.edu.cn/lufeng/en/index.htm). Previously, I received my B.Eng degree in Computer Science from Xidian University in 2022.

<!-- **My research** include the intersection of machine learning, deep learning, pattern recognition, and statistical modeling/inference with applications for computer vision, computational photography, low-level vision, human-computer interaction, and AR/MR.  -->

<!-- **My research** aims to build multimodal, highly expressive, lifelike, and immersive interactive agents, covering perception, understanding, reconstruction, and generation of *humans and the world*. *Specifically*: 

1. ğŸ‘ Scene perception and enhancement: [USI3D](https://liuyunfei.net/projects/cvpr20/index.html), [SGRRN](https://dl.acm.org/doi/10.1145/3510821), [SILS](https://arxiv.org/abs/1906.00734).

2. ğŸ‘€ Humanâ€“environment interaction perception: [PnP-GA](https://liuyunfei.net/projects/iccv21/index.html), [GazeOnce](https://github.com/mf-zhang/GazeOnce), [WISWYS](https://ieeexplore.ieee.org/document/9220850).

3. ğŸ˜ 2D/3D Head and body reconstruction, animation, and generation: [TEASER](https://tinyurl.com/TEASER-project), [GUAVA](https://eastbeanzhang.github.io/GUAVA/), [GPAvatar](https://xg-chu.site/project_gpavatar/), [MODA](https://liuyunfei.net/projects/iccv23-moda/index.html), [HRAvatar](https://eastbeanzhang.github.io/HRAvatar/), [DiffSHEG](https://jeremycjm.github.io/proj/DiffSHEG), [TokenFace](https://liuyunfei.net/).

4. ğŸ Image/Video editing and generation: [Qffusion](https://www.computer.org/csdl/journal/tg/5555/01/11106190/28NPQuwUsaA), [STEM-inv](https://stem-inv.github.io/page/), [AddMe](https://addme-awesome.github.io/page/).

Previously, I also worked on network interpretability AR/VR, and industrial anomaly detection: [Refool](https://arxiv.org/abs/2007.02343), [EGNIA](https://github.com/zhaoyuhsin/Edge-Guided-Near-Eye-Image-Analysis-for-Head-Mounted-Displays), [3DEG](https://ieeexplore.ieee.org/document/9756767), [UTAD](https://arxiv.org/pdf/2103.11671.pdf). -->

<!-- I serve as a reviewer for international conferences and journals, *e.g.*, CVPR, ICCV, NeuIPS, ICLR, ICML, ACM MM, TPAMI, IJCV, PR, TVCG, *etc.*. -->
<!-- 
> ğŸ‘ We are currently looking for self-motivated interns to explore cutting-edge techniques such as Gaussian Splatting and DM/FM. Feel free to [contact me](mailto:liuyunfei@idea.edu.cn) if you are interested. [zhihu](https://zhuanlan.zhihu.com/p/582929545) -->
